# Multilingual Text Classification with Siamese Neural Network

## Overview
This project implements a Siamese neural network using the XLM-RoBERTa transformer to handle multilingual text data, significantly improving cross-lingual text classification tasks.

## Features
- **Multilingual Text Handling**: Utilizes XLM-RoBERTa transformer for processing and tokenizing large-scale multilingual datasets.
- **Feature Engineering**: Handles sequences up to 512 tokens, ensuring robust input representation.
- **Transfer Learning**: Adapts pre-trained language models for toxic comment classification across diverse linguistic contexts.
- **High Performance**: Achieved an accuracy of 92.5%, precision of 91.3%, recall of 90.8%, and an F1-score of 91.0% on the test dataset.

## Results
The model demonstrated the following performance on the test dataset:
- **Accuracy**: 92.5%
- **Precision**: 91.3%
- **Recall**: 90.8%
- **F1-score**: 91.0%

## Contributing
Contributions are welcome! Please open an issue or submit a pull request.

## License
This project is licensed under the MIT License. See the LICENSE file for details.

## Acknowledgements
- Thanks to the developers of XLM-RoBERTa for their powerful multilingual transformer model.
- Special thanks to the open-source community for providing valuable resources and tools.

